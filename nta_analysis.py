"""
NTA DATA ANALYSIS FRAMEWORK - CORE FUNCTIONS ONLY

Comprehensive program for analyzing Nanoparticle Tracking Analysis (NTA) data
generated by ZetaView (Particle Metrix QUATT, ZetaView version 8.06.01 SP1).

This is a clean, importable module with function definitions only.
All cell-level executable code has been removed.
"""

import os
import re
import json
import ntpath
from datetime import date
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from matplotlib.lines import Line2D
from matplotlib.ticker import LogLocator, LogFormatter

from scipy import integrate
from scipy import stats as scipy_stats
from scipy.optimize import curve_fit, minimize

# ============================================================================
# CONFIGURATION
# ============================================================================

CONFIG = {
    "directory": "data",
    "file_identifier": ".txt",
    "output_subdirs": ["metadata", "processed"],
    "nta_concentration_calibration_factor": 4.61E+5,
    "project_metadata": {
        "experimenter": "Your_Initials",
        "location": "Your_Lab_Location",
        "project": "Your_Project_Name",
        "meta_version": "v03",
        "pi": "Principal_Investigator_Initials",
        "funding": "Funding_Source",
        "data_collection_method": "NTA",
        "unit_of_analysis": '["nm", "nm^2", "nm^3"]',
        "keywords": "particle_size_distribution",
        "publications": "None",
    }
}

# ============================================================================
# FILE I/O FUNCTIONS
# ============================================================================

def read_nta_file(filepath):
    """Read an NTA data file with appropriate encoding."""
    try:
        with open(filepath, 'r', encoding='latin1') as file:
            content = file.read()
        
        if not content or len(content) < 100:
            return False, f"File appears to be empty or too small: {filepath}"
        
        return True, content
    except Exception as e:
        return False, f"Error reading file: {str(e)}"


def identify_sections(content):
    """Identify key data sections in the file content."""
    sections = {}
    
    size_lin_start = content.find("Size Distribution")
    if size_lin_start == -1:
        return False, "Could not find 'Size Distribution' section for linear data"
    sections['linear_start'] = size_lin_start
    
    size_log_start = content.find("-1.000E+0")
    if size_log_start == -1:
        second_header = content.find("Size / nm", size_lin_start + 100)
        if second_header == -1:
            return False, "Could not find logarithmic data section"
        sections['logarithmic_start'] = second_header
    else:
        sections['logarithmic_start'] = size_log_start
    
    if sections['linear_start'] >= sections['logarithmic_start']:
        return False, "Invalid file structure: linear section should come before logarithmic section"
    
    return True, sections


def extract_data_section(content, start_pos, end_pos=None, is_log_section=False):
    """Extract tabular data from a section of the file content."""
    section = content[start_pos:end_pos]
    
    if is_log_section:
        sep_pos = section.find("-1.000E+0")
        if sep_pos != -1:
            sep_line_end = section.find('\n', sep_pos)
            if sep_line_end != -1:
                section = section[sep_line_end + 1:]
            else:
                section = section[sep_pos + 20:]
        
        second_header = section.find("Size Distribution")
        if second_header != -1:
            section = section[second_header:]
    
    header_match = re.search(r'Size / nm\s+Number\s+Concentration.+', section)
    
    if not header_match and is_log_section:
        header_match = re.search(r'Size.*Number', section)
        if not header_match:
            header_line = "Size / nm\tNumber\tConcentration / cm-3\tVolume / nm^3\tArea / nm^2"
            data_lines = []
            for line in section.split('\n'):
                if "-1.000E+0" in line:
                    continue
                if re.match(r'^\s*[\d.-]+E[\+\-]\d+\s+[\d.-]+E[\+\-]\d+', line):
                    data_lines.append(line)
            
            if not data_lines:
                return False, "Could not find any valid data lines in logarithmic section"
            return True, (header_line, data_lines)
        else:
            header_line = header_match.group(0)
            data_start = section.find(header_line) + len(header_line)
            data_section = section[data_start:]
    else:
        if header_match:
            header_line = header_match.group(0)
            data_start = section.find(header_line) + len(header_line)
            data_section = section[data_start:]
        else:
            return False, "Could not find header line in data section"
    
    all_lines = data_section.split('\n')
    data_lines = []
    
    for line in all_lines:
        if "-1.000E+0" in line:
            continue
        if re.match(r'^\s*[\d.-]+E[\+\-]\d+\s+[\d.-]+E[\+\-]\d+', line):
            data_lines.append(line)
        elif len(data_lines) > 0 and (line.strip() == '' or line.strip().startswith('-')):
            break
    
    if not data_lines:
        return False, "No data lines found in section"
    
    return True, (header_line, data_lines)


def parse_data_lines(header_line, data_lines, scale_type):
    """Parse data lines into a structured DataFrame."""
    parsed_data = []
    
    for line in data_lines:
        if "-1.000E+0" in line:
            continue
            
        values = re.findall(r'[\d.-]+E[\+\-]\d+', line)
        
        if len(values) >= 5:
            try:
                if values[0] == "-1.000E+0":
                    continue
                
                row = {
                    'size_nm': float(values[0]),
                    'number': float(values[1]),
                    'concentration_cm-3': float(values[2]),
                    'volume_nm^3': float(values[3]),
                    'area_nm^2': float(values[4]),
                    'scale': scale_type
                }
                parsed_data.append(row)
            except ValueError:
                continue
    
    if not parsed_data:
        return False, f"Failed to parse any data lines for {scale_type} scale"
    
    return True, pd.DataFrame(parsed_data)


def extract_single_file_distribution(content, sections, filename):
    """Extract distribution data from a single file."""
    lin_start = sections['linear_start']
    log_start = sections['logarithmic_start']
    
    success, lin_result = extract_data_section(content, lin_start, log_start, is_log_section=False)
    if not success:
        return False, f"Failed to extract linear data from {filename}: {lin_result}"
    
    lin_header, lin_data_lines = lin_result
    success, lin_df = parse_data_lines(lin_header, lin_data_lines, 'linear')
    if not success:
        return False, f"Failed to parse linear data from {filename}: {lin_df}"
    
    success, log_result = extract_data_section(content, log_start, is_log_section=True)
    if not success:
        combined_df = lin_df.copy()
    else:
        log_header, log_data_lines = log_result
        success, log_df = parse_data_lines(log_header, log_data_lines, 'logarithmic')
        if not success:
            combined_df = lin_df.copy()
        else:
            combined_df = pd.concat([lin_df, log_df], ignore_index=True)
    
    combined_df['source_file'] = filename
    
    return True, combined_df


def average_replicate_data(dataframes_list, filenames_list):
    """Average distribution data across multiple replicates bin-by-bin."""
    if not dataframes_list:
        return False, "No dataframes to average"
    
    if len(dataframes_list) == 1:
        df = dataframes_list[0].copy()
        
        df['number_sd'] = 0.0
        df['concentration_cm-3_sd'] = 0.0
        df['volume_nm^3_sd'] = 0.0
        df['area_nm^2_sd'] = 0.0
        
        df.rename(columns={
            'number': 'number_avg',
            'concentration_cm-3': 'concentration_cm-3_avg',
            'volume_nm^3': 'volume_nm^3_avg',
            'area_nm^2': 'area_nm^2_avg'
        }, inplace=True)
        
        df['num_replicates'] = 1
        df['source_files'] = filenames_list[0]
        
        return True, df
    
    averaged_dfs = []
    
    for scale in ['linear', 'logarithmic']:
        scale_dfs = []
        for df in dataframes_list:
            scale_data = df[df['scale'] == scale]
            if not scale_data.empty:
                scale_dfs.append(scale_data)
        
        if not scale_dfs:
            continue
        
        all_sizes = []
        for df in scale_dfs:
            all_sizes.extend(df['size_nm'].values)
        unique_sizes = sorted(set(all_sizes))
        
        averaged_data = []
        
        for size in unique_sizes:
            numbers = []
            concentrations = []
            volumes = []
            areas = []
            
            for df in scale_dfs:
                size_row = df[df['size_nm'] == size]
                if not size_row.empty:
                    numbers.append(size_row['number'].iloc[0])
                    concentrations.append(size_row['concentration_cm-3'].iloc[0])
                    volumes.append(size_row['volume_nm^3'].iloc[0])
                    areas.append(size_row['area_nm^2'].iloc[0])
            
            if numbers:
                row = {
                    'size_nm': size,
                    'number_avg': np.mean(numbers),
                    'number_sd': np.std(numbers, ddof=1) if len(numbers) > 1 else 0.0,
                    'concentration_cm-3_avg': np.mean(concentrations),
                    'concentration_cm-3_sd': np.std(concentrations, ddof=1) if len(concentrations) > 1 else 0.0,
                    'volume_nm^3_avg': np.mean(volumes),
                    'volume_nm^3_sd': np.std(volumes, ddof=1) if len(volumes) > 1 else 0.0,
                    'area_nm^2_avg': np.mean(areas),
                    'area_nm^2_sd': np.std(areas, ddof=1) if len(areas) > 1 else 0.0,
                    'scale': scale,
                    'num_replicates': len(numbers),
                    'source_files': '; '.join(filenames_list)
                }
                averaged_data.append(row)
        
        if averaged_data:
            scale_df = pd.DataFrame(averaged_data)
            averaged_dfs.append(scale_df)
    
    if not averaged_dfs:
        return False, "No data could be averaged"
    
    final_df = pd.concat(averaged_dfs, ignore_index=True)
    final_df = final_df.sort_values(['scale', 'size_nm']).reset_index(drop=True)
    
    return True, final_df


# ============================================================================
# METADATA EXTRACTION
# ============================================================================

def extract_all_metadata_fields(content, filename):
    """Extract ALL possible metadata fields from file content."""
    metadata_patterns = [
        ('original_file', r'Original File:\s+(.+?)(?:\s+Section:|$)'),
        ('section', r'Section:\s+(.+)'),
        ('operator', r'Operator:\s+(.+)'),
        ('experiment', r'Experiment:\s+(.+)'),
        ('zetaview_sn', r'ZetaView S/N:\s+(.+)'),
        ('cell_sn', r'Cell S/N:\s+(.+)'),
        ('software', r'Software:\s+(.+?)(?:\s+Analyze:|$)'),
        ('analyze', r'Analyze:\s+(.+)'),
        ('sop', r'SOP:\s+(.+)'),
        ('sample', r'Sample:\s+(.+)'),
        ('electrolyte', r'Electrolyte:(?:\s*(.*?))?(?:\r?\n|$)'),
        ('ph', r'pH:\s+(.+?)(?:\s+entered|$)'),
        ('conductivity', r'Conductivity:\s+(.+?)(?:\s+sensed|$)'),
        ('temp_control', r'TempControl:\s+(.+)'),
        ('set_temperature', r'SetTemperature:\s+(.+)'),
        ('temperature', r'Temperature:\s+(.+?)(?:\s+sensed|$)'),
        ('viscosity', r'Viscosity:\s+(.+)'),
        ('date', r'Date:\s+(.+)'),
        ('time', r'Time:\s+(.+)'),
        ('general_remarks', r'General Remarks:\s+(.+)'),
        ('remarks', r'Remarks:\s+(.+)'),
        ('sample_info_1', r'Sample Info 1:\s+(.+)'),
        ('sample_info_2', r'Sample Info 2:\s+(.+)'),
        ('sample_info_3', r'Sample Info 3:\s*(.*)'),
        ('scattering_intensity', r'Scattering Intensity:\s+(.+)'),
        ('detected_particles', r'Detected Particles:\s+(.+)'),
        ('particle_drift_checked', r'Particle Drift Checked:\s+(.+)'),
        ('particle_drift_check_result', r'Particle Drift Check Result:\s+(.+)'),
        ('cell_check_date', r'Cell Checked:\s+(\d{4}-\d{2}-\d{2})'),
        ('cell_check_result', r'Cell Check Result:\s+(.+)'),
        ('type_of_measurement', r'Type of Measurement:\s+(.+)'),
        ('positions', r'Positions:\s+(.+)'),
        ('microscope_position', r'Microscope Position:\s+(.+)'),
        ('number_of_traces', r'Number of Traces:\s+(\d+)'),
        ('average_number_of_particles', r'Average Number of Particles:\s+(\d+\.\d+)'),
        ('dilution', r'Dilution::\s+(\d+\.\d+)'),
        ('concentration_correction_factor', r'Concentration Correction Factor:\s+(.+)'),
        ('laser_wavelength', r'Laser Wavelength nm:\s+(\d+\.\d+)'),
        ('median_number_d50', r'Median Number \(D50\):\s+(.+)'),
        ('median_concentration_d50', r'Median Concentration \(D50\):\s+(.+)'),
        ('median_volume_d50', r'Median Volume \(D50\):\s+(.+)'),
        ('minimum_brightness', r'Minimum Brightness:\s+(\d+)'),
        ('minimum_area', r'Minimum Area:\s+(\d+)'),
        ('maximum_area', r'Maximum Area:\s+(\d+)'),
        ('maximum_brightness', r'Maximum Brightness:\s+(\d+)'),
        ('tracking_radius2', r'Tracking Radius2:\s+(\d+)'),
        ('minimum_tracelength', r'Minimum Tracelength:\s+(\d+)'),
        ('fps', r'Camera:\s*FpSec\s+(\d+)\s+#Cycles'),
        ('cycles', r'#Cycles\s+(\d+)'),
        ('camera_settings', r'Camera:\s+(.+)'),
        ('frame_rate', r'FRate\s+(\d+\.\d+)'),
    ]
    
    metadata = {}
    
    for key, pattern in metadata_patterns:
        match = re.search(pattern, content)
        if match:
            value = match.group(1).strip() if match.group(1) else ''
            if value and not value.lower() in ['none', 'null', '']:
                metadata[key] = value
    
    metadata['filename'] = filename
    
    base_name = os.path.splitext(filename)[0]
    if base_name.endswith("_rawdata"):
        base_name = base_name[:-8]
    if base_name.startswith("Data_"):
        base_name = base_name[5:]
    metadata['uniqueID'] = base_name
    
    return metadata


def extract_metadata_from_all_files(files_data):
    """Extract metadata from all files and organize by filename."""
    if not files_data:
        return False, "No files provided for metadata extraction"
    
    all_files_metadata = {}
    
    for filename, content, sections in files_data:
        metadata = extract_all_metadata_fields(content, filename)
        all_files_metadata[filename] = metadata
    
    return True, all_files_metadata


def analyze_field_differences(all_files_metadata):
    """Analyze which fields are identical vs. different across files."""
    if not all_files_metadata:
        return {}, {}, {}
    
    all_field_names = set()
    for metadata in all_files_metadata.values():
        all_field_names.update(metadata.keys())
    
    identical_fields = {}
    different_fields = {}
    field_analysis = {}
    
    for field_name in sorted(all_field_names):
        values = []
        files_with_field = []
        
        for filename, metadata in all_files_metadata.items():
            if field_name in metadata:
                values.append(metadata[field_name])
                files_with_field.append(filename)
        
        field_info = {
            'values': values,
            'files_with_field': files_with_field,
            'present_in_files': len(files_with_field),
            'total_files': len(all_files_metadata)
        }
        
        if len(set(values)) == 1:
            identical_fields[field_name] = values[0]
            field_info['status'] = 'identical'
        else:
            different_fields[field_name] = values
            field_info['status'] = 'different'
        
        field_analysis[field_name] = field_info
    
    return identical_fields, different_fields, field_analysis


def smart_format_field(field_name, values):
    """Apply smart formatting rules based on field type and content."""
    file_specific_fields = [
        'filename', 'avi_filename', 'experiment', 'original_file', 'uniqueID', 
        'time', 'particle_drift_checked'
    ]
    
    text_first_fields = [
        'sample_info_1', 'sample_info_2', 'sample_info_3', 'remarks', 'general_remarks'
    ]
    
    sum_fields = [
        'number_of_traces', 'detected_particles'
    ]
    
    quality_control_fields = [
        'particle_drift_check_result', 'cell_check_result'
    ]
    
    notes = ""
    
    if field_name in file_specific_fields:
        return json.dumps(values), "file_specific"
    elif field_name in text_first_fields:
        return values[0], f"using_first_of_{len(values)}"
    elif field_name in sum_fields:
        try:
            numeric_values = [float(v) for v in values]
            total = sum(numeric_values)
            return f"{total:.0f}", f"sum_of_{len(values)}_measurements"
        except ValueError:
            return json.dumps(values), "non_numeric_sum_field"
    elif field_name in quality_control_fields:
        unique_values = list(set(values))
        if len(unique_values) == 1:
            return values[0], "qc_consistent"
        else:
            return json.dumps(values), f"QC_ALERT_inconsistent_values"
    else:
        try:
            numeric_values = [float(v) for v in values]
            mean_val = np.mean(numeric_values)
            std_val = np.std(numeric_values, ddof=1) if len(numeric_values) > 1 else 0.0
            cv = (std_val / mean_val * 100) if mean_val != 0 else 0
            
            if cv > 10:
                notes = f"HIGH_VARIATION_CV_{cv:.1f}%"
            else:
                notes = f"mean_sd_of_{len(values)}"
            
            return f"{mean_val:.2f} Â± {std_val:.2f}", notes
        except ValueError:
            return json.dumps(values), "non_numeric_different"


def create_automated_metadata(all_files_metadata, identical_fields, different_fields, config=None):
    """Create essential standardized metadata for multi-file analysis."""
    filenames = list(all_files_metadata.keys())
    num_files = len(filenames)
    
    metadata = {}
    processing_notes = {}
    
    if 'uniqueID' in identical_fields:
        base_id = identical_fields['uniqueID']
    else:
        base_id = list(all_files_metadata.values())[0].get('uniqueID', 'unknown')
    
    if num_files > 1:
        unique_id = f"{base_id}_avg{num_files}"
    else:
        unique_id = base_id
    
    project_meta = config.get("project_metadata", {}) if config else {}
    
    metadata['experimenter'] = project_meta.get('experimenter', 'SH/HB')
    metadata['location'] = project_meta.get('location', 'HD_MPImF_CBP_R0.106')
    metadata['project'] = project_meta.get('project', 'LEAF')
    metadata['meta_version'] = project_meta.get('meta_version', 'v02')
    metadata['pi'] = project_meta.get('pi', 'HB')
    metadata['funding'] = project_meta.get('funding', 'MPG')
    metadata['persistentID'] = unique_id
    metadata['data_collection_method'] = project_meta.get('data_collection_method', 'NTA')
    metadata['nta_instrument'] = 'ZetaView'
    
    if 'analyze' in identical_fields:
        metadata['nta_software'] = f"ZetaView {identical_fields['analyze']}"
    elif 'analyze' in different_fields:
        metadata['nta_software'] = f"ZetaView {different_fields['analyze'][0]}"
    else:
        metadata['nta_software'] = 'ZetaView'
    
    metadata['nta_processed_file'] = f"Data_{unique_id}_PSD.txt"
    
    if 'sample' in identical_fields:
        metadata['sample'] = identical_fields['sample']
    elif 'sample' in different_fields:
        metadata['sample'] = different_fields['sample'][0]
    
    metadata['num_replicates'] = num_files
    metadata['source_files'] = json.dumps(filenames)
    
    essential_fields = {
        'date', 'temperature', 'ph', 'dilution', 'laser_wavelength', 'electrolyte',
        'positions', 'cycles', 'fps', 
        'particle_drift_check_result', 'cell_check_result',
        'average_number_of_particles', 'number_of_traces', 'detected_particles',
        'conductivity', 'scattering_intensity', 'viscosity',
        'avi_filesize'
    }
    
    for field_name, value in identical_fields.items():
        if field_name in essential_fields:
            if field_name in ['temperature', 'ph', 'dilution', 'laser_wavelength', 'positions', 
                             'cycles', 'fps', 'average_number_of_particles', 'number_of_traces', 
                             'detected_particles', 'particle_drift_check_result', 'cell_check_result',
                             'conductivity', 'scattering_intensity', 'viscosity', 'avi_filesize']:
                if field_name in ['number_of_traces', 'detected_particles']:
                    metadata[f'nta_{field_name}_sum'] = value
                else:
                    metadata[f'nta_{field_name}'] = value
            else:
                metadata[field_name] = value
    
    quality_alerts = []
    high_variation_fields = []
    
    for field_name, values in different_fields.items():
        if field_name in essential_fields:
            formatted_value, notes = smart_format_field(field_name, values)
            
            if "QC_ALERT" in notes:
                quality_alerts.append(f"{field_name}: {formatted_value}")
            if "HIGH_VARIATION" in notes:
                high_variation_fields.append(f"{field_name}: {notes}")
            
            if field_name in ['temperature', 'ph', 'dilution', 'laser_wavelength', 'positions', 
                             'cycles', 'fps', 'average_number_of_particles', 'number_of_traces', 
                             'detected_particles', 'particle_drift_check_result', 'cell_check_result',
                             'conductivity', 'scattering_intensity', 'viscosity', 'avi_filesize']:
                if field_name in ['number_of_traces', 'detected_particles']:
                    metadata[f'nta_{field_name}_sum'] = formatted_value
                else:
                    metadata[f'nta_{field_name}'] = formatted_value
            else:
                metadata[field_name] = formatted_value
            
            processing_notes[field_name] = notes
    
    metadata['python_analysis'] = str(date.today())
    
    if quality_alerts:
        metadata['quality_control_alerts'] = json.dumps(quality_alerts)
    
    if high_variation_fields:
        metadata['high_variation_fields'] = json.dumps(high_variation_fields)
    
    return metadata


# ============================================================================
# CORE CALCULATIONS
# ============================================================================

def apply_dilution_correction_with_uncertainty(df, metadata=None, manual_dilution=None):
    """Apply dilution correction to all measured values with uncertainty propagation."""
    updated_df = df.copy()
    
    dilution_factor = 1.0
    dilution_source = "default (no dilution)"
    
    if manual_dilution is not None:
        try:
            dilution_factor = float(manual_dilution)
            dilution_source = "manually specified"
        except (ValueError, TypeError):
            return False, f"Invalid manual dilution factor: {manual_dilution}"
    
    elif metadata is not None:
        if 'nta_dilution' in metadata:
            try:
                dilution_string = metadata['nta_dilution']
                dilution_factor = float(dilution_string.split('Â±')[0].strip()) if 'Â±' in dilution_string else float(dilution_string)
                dilution_source = "metadata (nta_dilution)"
            except (ValueError, TypeError):
                pass
    
    if 'concentration_cm-3_avg' in updated_df.columns:
        updated_df['concentration_cm-3_per_mL_avg'] = updated_df['concentration_cm-3_avg'] * dilution_factor
        if 'concentration_cm-3_sd' in updated_df.columns:
            updated_df['concentration_cm-3_per_mL_sd'] = updated_df['concentration_cm-3_sd'] * dilution_factor
        
        updated_df = updated_df.drop(['concentration_cm-3_avg', 'concentration_cm-3_sd'], axis=1, errors='ignore')
    else:
        return False, "Missing concentration_cm-3_avg column for dilution correction"
    
    if 'volume_nm^3_avg' in updated_df.columns:
        updated_df['volume_nm^3_per_mL_avg'] = updated_df['volume_nm^3_avg'] * dilution_factor
        if 'volume_nm^3_sd' in updated_df.columns:
            updated_df['volume_nm^3_per_mL_sd'] = updated_df['volume_nm^3_sd'] * dilution_factor
        
        updated_df = updated_df.drop(['volume_nm^3_avg', 'volume_nm^3_sd'], axis=1, errors='ignore')
    
    if 'area_nm^2_avg' in updated_df.columns:
        updated_df['area_nm^2_per_mL_avg'] = updated_df['area_nm^2_avg'] * dilution_factor
        if 'area_nm^2_sd' in updated_df.columns:
            updated_df['area_nm^2_per_mL_sd'] = updated_df['area_nm^2_sd'] * dilution_factor
        
        updated_df = updated_df.drop(['area_nm^2_avg', 'area_nm^2_sd'], axis=1, errors='ignore')
    
    return True, updated_df


def normalize_distributions_with_uncertainty(df, metadata=None, size_column='size_nm'):
    """Normalize particle distributions by area under the curve with uncertainty propagation."""
    normalized_df = df.copy()
    
    for scale in normalized_df['scale'].unique():
        scale_mask = normalized_df['scale'] == scale
        scale_data = normalized_df[scale_mask].copy()
        
        if scale_data.empty or 'number_avg' not in scale_data.columns:
            continue
        
        scale_data = scale_data.sort_values(size_column)
        
        sizes = scale_data[size_column].values
        numbers_avg = scale_data['number_avg'].values
        
        if len(sizes) < 2:
            continue
        
        area_avg = np.trapz(numbers_avg, sizes)
        
        if area_avg > 0:
            normalized_df.loc[scale_mask, 'number_normalized_avg'] = numbers_avg / area_avg
            
            if 'number_sd' in scale_data.columns:
                numbers_sd = scale_data['number_sd'].values
                normalized_df.loc[scale_mask, 'number_normalized_sd'] = numbers_sd / area_avg
            else:
                normalized_df.loc[scale_mask, 'number_normalized_sd'] = 0.0
        else:
            normalized_df.loc[scale_mask, 'number_normalized_avg'] = 0.0
            normalized_df.loc[scale_mask, 'number_normalized_sd'] = 0.0
    
    return True, normalized_df


def calculate_cumulative_distributions_with_uncertainty(df, scale_column='scale'):
    """Calculate cumulative distributions with proper uncertainty propagation."""
    result_df = df.copy()
    
    for scale in result_df[scale_column].unique():
        scale_mask = result_df[scale_column] == scale
        scale_indices = result_df[scale_mask].sort_values('size_nm').index
        
        if len(scale_indices) == 0:
            continue
        
        if 'number_normalized_avg' in result_df.columns:
            cumsum_avg = result_df.loc[scale_indices, 'number_normalized_avg'].cumsum()
            
            if cumsum_avg.iloc[-1] > 0:
                result_df.loc[scale_indices, 'number_normalized_cumsum_avg'] = cumsum_avg / cumsum_avg.iloc[-1]
            else:
                result_df.loc[scale_indices, 'number_normalized_cumsum_avg'] = 0
            
            if 'number_normalized_sd' in result_df.columns:
                normalized_var_cumsum = (result_df.loc[scale_indices, 'number_normalized_sd'] ** 2).cumsum()
                cumsum_sd = np.sqrt(normalized_var_cumsum)
                
                if cumsum_avg.iloc[-1] > 0:
                    result_df.loc[scale_indices, 'number_normalized_cumsum_sd'] = cumsum_sd / cumsum_avg.iloc[-1]
                else:
                    result_df.loc[scale_indices, 'number_normalized_cumsum_sd'] = 0
        
        if 'volume_nm^3_per_mL_avg' in result_df.columns:
            cumsum_avg = result_df.loc[scale_indices, 'volume_nm^3_per_mL_avg'].cumsum()
            result_df.loc[scale_indices, 'volume_nm^3_per_mL_cumsum_avg'] = cumsum_avg
            
            if 'volume_nm^3_per_mL_sd' in result_df.columns:
                volume_var_cumsum = (result_df.loc[scale_indices, 'volume_nm^3_per_mL_sd'] ** 2).cumsum()
                result_df.loc[scale_indices, 'volume_nm^3_per_mL_cumsum_sd'] = np.sqrt(volume_var_cumsum)
        
        if 'area_nm^2_per_mL_avg' in result_df.columns:
            cumsum_avg = result_df.loc[scale_indices, 'area_nm^2_per_mL_avg'].cumsum()
            result_df.loc[scale_indices, 'area_nm^2_per_mL_cumsum_avg'] = cumsum_avg
            
            if 'area_nm^2_per_mL_sd' in result_df.columns:
                area_var_cumsum = (result_df.loc[scale_indices, 'area_nm^2_per_mL_sd'] ** 2).cumsum()
                result_df.loc[scale_indices, 'area_nm^2_per_mL_cumsum_sd'] = np.sqrt(area_var_cumsum)
    
    return True, result_df


def interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, target_fraction):
    """Calculate D-value with asymmetric confidence bounds."""
    sorted_indices = np.argsort(sizes)
    sizes_sorted = sizes[sorted_indices]
    cumsum_avg_sorted = cumsum_avg[sorted_indices]
    cumsum_sd_sorted = cumsum_sd[sorted_indices]
    
    cumsum_lower = cumsum_avg_sorted - cumsum_sd_sorted
    cumsum_upper = cumsum_avg_sorted + cumsum_sd_sorted
    
    cumsum_avg_sorted = np.clip(cumsum_avg_sorted, 0, 1)
    cumsum_lower = np.clip(cumsum_lower, 0, 1)
    cumsum_upper = np.clip(cumsum_upper, 0, 1)
    
    cumsum_avg_sorted = np.maximum.accumulate(cumsum_avg_sorted)
    cumsum_lower = np.maximum.accumulate(cumsum_lower)
    cumsum_upper = np.maximum.accumulate(cumsum_upper)
    
    if target_fraction < cumsum_avg_sorted[0] or target_fraction > cumsum_avg_sorted[-1]:
        return np.nan, np.nan, np.nan
    
    try:
        d_value_avg = np.interp(target_fraction, cumsum_avg_sorted, sizes_sorted)
        d_value_lower = np.interp(target_fraction, cumsum_upper, sizes_sorted)
        d_value_upper = np.interp(target_fraction, cumsum_lower, sizes_sorted)
        
        return d_value_avg, d_value_lower, d_value_upper
    except Exception:
        return np.nan, np.nan, np.nan


def calculate_percentile_statistics_with_uncertainty(df, size_column='size_nm'):
    """Calculate percentile statistics (D10, D50, D90, span) with uncertainties."""
    if df is None or df.empty:
        return False, "Input dataframe is empty"
    
    stats = {'linear': {}, 'logarithmic': {}}
    
    cumsum_configs = [
        {
            'name': 'number',
            'avg_column': 'number_normalized_cumsum_avg',
            'sd_column': 'number_normalized_cumsum_sd'
        },
        {
            'name': 'volume',
            'avg_column': 'volume_nm^3_per_mL_cumsum_avg', 
            'sd_column': 'volume_nm^3_per_mL_cumsum_sd'
        },
        {
            'name': 'surface_area',
            'avg_column': 'area_nm^2_per_mL_cumsum_avg',
            'sd_column': 'area_nm^2_per_mL_cumsum_sd'
        }
    ]
    
    for scale in ['linear', 'logarithmic']:
        scale_df = df[df['scale'] == scale].copy()
        if scale_df.empty:
            continue
        
        scale_df = scale_df.sort_values(size_column)
        
        for config in cumsum_configs:
            name = config['name']
            avg_column = config['avg_column']
            sd_column = config['sd_column']
            
            if avg_column not in scale_df.columns or sd_column not in scale_df.columns:
                continue
            
            sizes = scale_df[size_column].values
            cumsum_avg = scale_df[avg_column].values
            cumsum_sd = scale_df[sd_column].values
            
            if np.all(cumsum_avg == 0) or np.all(np.isnan(cumsum_avg)):
                continue
            
            if name in ['volume', 'surface_area']:
                max_cumsum = np.nanmax(cumsum_avg)
                if max_cumsum > 0:
                    cumsum_avg = cumsum_avg / max_cumsum
                    cumsum_sd = cumsum_sd / max_cumsum
                else:
                    continue
            
            try:
                d10_avg, d10_lower, d10_upper = interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, 0.1)
                d50_avg, d50_lower, d50_upper = interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, 0.5)
                d90_avg, d90_lower, d90_upper = interpolate_d_value_with_bounds(sizes, cumsum_avg, cumsum_sd, 0.9)
                
                if not np.isnan(d10_avg) and not np.isnan(d50_avg) and not np.isnan(d90_avg) and d50_avg > 0:
                    span_avg = (d90_avg - d10_avg) / d50_avg
                    
                    possible_spans = [
                        (d90_lower - d10_upper) / d50_upper,
                        (d90_lower - d10_upper) / d50_lower,
                        (d90_upper - d10_lower) / d50_upper,
                        (d90_upper - d10_lower) / d50_lower
                    ]
                    
                    valid_spans = [s for s in possible_spans if not np.isnan(s) and np.isfinite(s)]
                    
                    if valid_spans:
                        span_lower = min(valid_spans)
                        span_upper = max(valid_spans)
                    else:
                        span_lower, span_upper = np.nan, np.nan
                else:
                    span_avg, span_lower, span_upper = np.nan, np.nan, np.nan
                
                metrics = {
                    'D10_avg': d10_avg,
                    'D10_lower': d10_lower,
                    'D10_upper': d10_upper,
                    'D50_avg': d50_avg,
                    'D50_lower': d50_lower,
                    'D50_upper': d50_upper,
                    'D90_avg': d90_avg,
                    'D90_lower': d90_lower,
                    'D90_upper': d90_upper,
                    'span_avg': span_avg,
                    'span_lower': span_lower,
                    'span_upper': span_upper
                }
                
                stats[scale][name] = metrics
                
            except Exception:
                continue
    
    return True, stats
